#!/usr/bin/env node

const usage = `
Usage: ${process.argv[1].split('/').slice(-1)[0]} [option] inputfile

Options:
  -h or --help  Show this help message and exit
  -c            prints the token count for the input file
  -e            prints a quick estimation of the token count for the input file (faster than -c)
  -n            prints the token numbers for the input file separated by one space
  -nv           prints the tokens for the input as token number, tab, token, newline
  -d            the input file must be a whitespace separated token list; decodes that into the original text and prints it
  -sm number    shortens the input file to at most the given number of tokens by cutting off the middle
  -ss number    shortens the input file to at most the given number of tokens by cutting off the start
  -se number    shortens the input file to at most the given number of tokens by cutting off the end

Description:
  There always has to be exactly one option given (possibly including a number) and the input file.
  This script uses tokenization according to ChatGPT-3.5 / ChatGPT-4 tokenization with cl100k_base .
  If the input file is given as - then the input is read from stdin.
  If the input file is shortened, the removed part is replaced by ' ... ' . If there is no need for shortening, the input is printed as is.
`;

const fs = require('fs');
const tokenToNumber = new Map();
const numberToToken = new Map();
const tokens = [];

function processLines(callback) {
    const fileContent = fs.readFileSync(__dirname + '/.cl100k_base.tiktoken', 'utf8');
    const lines = fileContent.split('\n');
    for (let i = 0; i < lines.length; i++) {
        if (lines[i] === '') continue;
        callback(lines[i]);
    }
}

function callback(line) {
    const parts = line.split(' ');
    const token = Buffer.from(parts[0], 'base64').toString('utf8');
    const number = parseInt(parts[1], 10); // Ensure the number is stored as an integer
    tokenToNumber.set(token, number);
    numberToToken.set(number, token); // Store the token against the number
}

/** Extremely simple and embarssing inefficient implementation of tokenization, but that doesn't matter as ChatGPT is much much slower. */
function tokenize(input) {
    let result = [];
    let pos = 0;
    while (pos < input.length) {
        let token = '';
        for (let i = 1; i <= 128 && pos + i <= input.length; i++) {
            let tokenCandidate = input.substring(pos, pos + i);
            if (tokenToNumber.has(tokenCandidate)) {
                token = tokenCandidate;
            }
        }
        if (token === '') {
            // no idea what ChatGPT does in this case. :-(
            // console.error('Warning: Character ignored, no token found at ', pos, ' of ', input.substring(pos).length, ' for first character of ', input.substring(pos, pos + 128));
            pos++;
            continue;
        }
        // console.log(token, tokenToNumber.get(token));
        pos += token.length;
        result.push(token);
    }
    return result;
}

function decode(tokenNums) {
    const tokens = tokenNums.trim().split(/\s+/);
    let result = '';
    for (let i = 0; i < tokens.length; i++) {
        const token = numberToToken.get(parseInt(tokens[i], 10));
        if (!token) {
            console.error('No token found for ', tokens[i]);
            process.exit(1);
        }
        result += token;
    }
    return result;
}

function shorten(input, numTokens, mode) {
    const tokenList = tokenize(input);
    if (tokenList.length <= numTokens) {
        return input;
    }
    let result = '';
    switch (mode) {
        case 'm': // Change the case options to match the ones passed ('m' for middle, 's' for start, 'e' for end)
            const half = Math.floor(numTokens / 2);
            result = [...tokenList.slice(0, half), '...', ...tokenList.slice(-half)].join('');
            break;
        case 's':
            result = '...' + tokenList.slice(tokenList.length - numTokens).join('');
            break;
        case 'e':
            result = tokenList.slice(0, numTokens).join('') + '...';
            break;
    }
    return result;
}

function readInput(fileArg) {
    return fileArg && fileArg !== '-' ? fs.readFileSync(fileArg, 'utf8') : fs.readFileSync(0, 'utf8');
}

// Main function that handles command-line options
function main() {
    const option = process.argv[2];

    switch (option) {
        case '-h':
        case '--help':
            console.log(usage);
            break;
        case '-c': {
            const input = readInput(process.argv[3]);
            console.log(tokenize(input).length);
            break;
        }
        case '-e': {
            const input = readInput(process.argv[3]);
            console.log(estimateTextTokens(input));
            break;
        }
        case '-n': {
            const input = readInput(process.argv[3]);
            console.log(tokenize(input).map(token => tokenToNumber.get(token)).join(' '));
            break;
        }
        case '-nv': {
            const input = readInput(process.argv[3]);
            tokenize(input).forEach(token => {
                console.log(`${tokenToNumber.get(token)}\t${token}`);
            });
            break;
        }
        case '-d': {
            const tokenNums = readInput(process.argv[3]);
            console.log(decode(tokenNums));
            break;
        }
        case '-sm':
        case '-ss':
        case '-se': {
            const numTokens = parseInt(process.argv[3], 10);
            const mode = option.slice(-1);
            const input = readInput(process.argv[4]);
            console.log(shorten(input, numTokens, mode));
            break;
        }
        // Now unofficial options that aren't in the usage but used for research
        case '-w': {
            // prints a quick estimation for the token count for the input file based on the word count (faster than -c but really really bad - use -e)
            const input = readInput(process.argv[3]);
            const words = input.split(/\s+/);
            console.log(Math.round(words.length * 4 / 3));
            break;
        }
        case '-charstats': {
            printCharacterTokenStatistics(tokenize(readInput(process.argv[3])));
            break;
        }
        case '-classstats': {
            printCharacterTokenStatistics(tokenize(readInput(process.argv[3])), characterclass);
            break;
        }
        default:
            console.error('Unknown option:', option);
            console.log(usage);
    }
}

const C0 = 'NORabcdefghilnopqrstuvy'; // plus space that is not following a space
const C1 = '"#%)\*+56789<>?@Z[\\]^|§«äç\'';
const C2 = '-.ABDEFGIKWY_\r\tz{ü';
const C3 = ',01234:~Üß'; // incl. unicode characters > 255
const C4 = ''; // space that is following a space
const C5 = '!$&(/;=JX`j\n}ö';
const C6 = 'CHLMPQSTUVfkmspwx ';

const allClusters = [C0, C1, C2, C3, C4, C5, C6];

/* Determines whether the character at position pos in the token belongs to which of these clusters, observing the spcnt, sp, ml, tab, ret, uni charcterizations like in characterclassmapper. It returns 'C0' to 'C6'. We minimize by using strings of characters for each cluster. */
function characterclass(token, pos) {
    const char = token[pos];
    if (char === ' ') {
        // first char is 'space' , if it's a char after a space it's 'spacecont'
        if (pos > 0 && token[pos - 1] === ' ') {
            return 'C4';
        } else {
            return 'C0';
        }
    } else if (char.charCodeAt(0) > 255) {
        return 'C3';
    }
    // find out which one of the clusters in `allClusters` contains the character
    for (let i = 0; i < allClusters.length; i++) {
        if (allClusters[i].indexOf(char) !== -1) {
            return 'C' + i;
        }
    }
    return 'CX';
}

const avgtokenPerClass = {
    'C4': 0.08086208692099685,
    'C0': 0.2020182639633662,
    'C6': 0.2372744211422125,
    'C2': 0.3042805747355606,
    'C5': 0.4157646363858563,
    'C1': 0.4790556468110302,
    'C3': 0.6581971122770317,
    'CX': 0.980083857442348
};

function estimateTextTokens(text) {
    let tokencount = 0;
    for (let i = 0; i < text.length; i++) {
        tokencount += avgtokenPerClass[characterclass(text, i)];
    }
    return Math.round(tokencount);
}

// "research" code

function printCharacterTokenStatistics(tokens, characterclassmapper = c => c) {
    // for each character that occurs in the input, we want to estimate the average number of tokens that are needed to encode it
    // that is, for each character in every token we calculate 1/length of token and the average that over all occurrences of the character
    // we need to count the number of occurrences of each character in the input and
    // the sum of the inverse of the length of each token that contains the character
    const charCounts = new Map();
    const charTokenRatioSum = new Map();
    for (let i = 0; i < tokens.length; i++) {
        const token = tokens[i];
        for (let j = 0; j < token.length; j++) {
            const char = characterclassmapper(token, j);
            charCounts.set(char, (charCounts.get(char) || 0) + 1);
            charTokenRatioSum.set(char, (charTokenRatioSum.get(char) || 0) + 1 / token.length);
        }
    }
    const charTokenRatios = new Map();
    for (const [char, count] of charCounts.entries()) {
        charTokenRatios.set(char, charTokenRatioSum.get(char) / count);
    }
    const sorted = [...charTokenRatios.entries()].sort((a, b) => a[1] - b[1]);
    for (const [char, ratio] of sorted) {
        // console.log(char, ratio, charCounts.get(char));
        // console.log(char, ratio);
        // also write bytes of char to stdout
        // console.log(ratio, char, char.charCodeAt(0));
        // console.log(ratio, char);
        console.log(ratio, char, charCounts.get(char));
    }
}

function characternamemapper(token) {
    const splitted = token.split('');
    const result = [];
    for (let i = 0; i < splitted.length; i++) {
        const char = splitted[i];
        if (char === ' ') {
            // first char is 'space' , if it's a char after a space it's 'spacecont'
            if (i > 0 && splitted[i - 1] === ' ') {
                result.push('spcnt');
            } else {
                result.push('sp');
            }
        } else if (char === '\n') {
            result.push('ml');
        } else if (char === '\t') {
            result.push('tab');
        } else if (char === '\r') {
            result.push('ret');
        } else if (char.charCodeAt(0) > 255) {
            result.push('uni');
        } else {
            result.push(char);
        }
    }
    return result;
}

// =============
// now, after all variables are defined, we can start the main program

processLines(callback);
main();

// results on some of my files
// (real token count) (estimation with -e) (4/3 number of words) (filename)
// 123491	103405	49453	allwithextension.css
// 232691	243483	76931	allwithextension.html
// 671616	757334	379725	allwithextension.java
// 838884	825870	377397	allwithextension.js
// 30054	28625	10501	allwithextension.jsp
// 60583	59638	40700	allwithextension.md
// 55594	56122	35131	allwithextension.properties
// 14965	14280	6749	allwithextension.scss
// 912672	857563	234644	allwithextension.xml
// 3304048	3304144	1455413	alltogether
