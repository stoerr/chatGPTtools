
handle

{
  "error": {
    "message": "This model's maximum context length is 4097 tokens. However, your messages resulted in 4946 tokens. Please reduce the length of the messages.",
    "type": "invalid_request_error",
    "param": "messages",
    "code": "context_length_exceeded"
  }
}
+ cutoff light
Ouch - even if it doesn't exceed that: we need room for the response!

bookmarklets to drag to bookmarks bar
choose element and grab text from it
scroll the answer if too long
show text in title of "include text", incl. prompt fragment.

perhaps:
grab text from previous element with focus
thinking spinner
insert into previous element with focus
